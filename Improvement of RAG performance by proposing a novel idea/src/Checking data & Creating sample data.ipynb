{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 14:03:05.320945: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 14:03:06.480230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'processed_wikipedia.txt/processed_wikipedia.txt'\n",
    "# with open(path, 'r') as f:\n",
    "#     wiki_data = f.readlines()\n",
    "    \n",
    "train_path = 'qa_train.json'\n",
    "with open(train_path,\"r\") as json_file:\n",
    "    train = json.load(json_file)\n",
    "\n",
    "test_path = 'qa_test.json'\n",
    "with open(test_path,\"r\") as json_file:\n",
    "    test = json.load(json_file)\n",
    "    \n",
    "# sample data\n",
    "# wiki_data_samp = wiki_data[:20]\n",
    "train_samp = train[:5]\n",
    "test_samp = test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fzla TAuseef (May 18, 1s36 – DecemBmer 30, 014 ), also spleled Afozal Tausif, was a PakIstani Pu2jabi lagnuage rit9re, coulnmist and jounalbsit.\n",
      "\n",
      "She criticiezd militay dctatorship in Pakstna and was detaide, altr dsplaced sereal tise by the urlers of that time cuh as Aybu Kan and Muhammad Zia - ul - Haq.\n",
      "\n",
      "Afzal has authZrd mIIe th! an rvhirty ooNs in 7untabi as wel as in U0du.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "with open(path,'r') as f:\n",
    "    for line in f:\n",
    "        if cnt < 3:\n",
    "            print(line.strip() + \"\\n\")\n",
    "            cnt += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': '1966',\n",
       "  'question': 'In what year the the venue that Marcia White is president of open?'},\n",
       " {'answer': 'Portugal',\n",
       "  'question': 'What country is home to the sports club loaning Bruno Paulista to Vasco da Gama?'}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Mike Huckabee took part in the most recent Republican Presidential primaries, which began on what 2016 date?'},\n",
       " {'question': 'Pabst Brewing Company is currently a holding contract company for what brew company whose main brand was marketed as The National Beer of Texas?'}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train :  88066\n",
      "length of test :  9786\n"
     ]
    }
   ],
   "source": [
    "print('length of train : ', len(train))\n",
    "print('length of test : ',len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Evaluating File\n",
    "- Running evaluate.py with these files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': {'0': 'The Eiffel Tower', '1': 'Paris', '2': 'France'}, 'sp': {'0': 'The cat chased the mouse.', '1': 'She loves to read books.', '2': 'The sun is shining brightly.'}}\n",
      "[{'_id': '0', 'answer': 'The Eiffel Tower', 'supporting_facts': 'They went to the beach.'}, {'_id': '1', 'answer': 'Korea', 'supporting_facts': 'He plays the guitar well.'}, {'_id': '2', 'answer': 'France', 'supporting_facts': 'The sun is shining brightly.'}]\n"
     ]
    }
   ],
   "source": [
    "## prediction -> qa_test와 wikipedia.txt로 생성\n",
    "# 질문의 id : 0,1,2 / 질문의 예측된 답변 : the eiffel tower, paris, france\n",
    "# q1의 supporting fact : The cat chased the mouse\n",
    "prediction = {\n",
    "  \"answer\": {\n",
    "    \"0\": \"The Eiffel Tower\",\n",
    "    \"1\": \"Paris\",\n",
    "    \"2\": \"France\"\n",
    "  },\n",
    "  \"sp\": {\n",
    "    \"0\": \"The cat chased the mouse.\",\n",
    "    \"1\": \"She loves to read books.\",\n",
    "    \"2\": \"The sun is shining brightly.\"\n",
    "  }\n",
    "}\n",
    "## gold -> qa_train으로 생성\n",
    "# 질문의 id : 0,1,2 / 질문의 실제 답변 : the eiffel tower, korea, france \n",
    "# q1의 supporting fact : They went to the beach.\n",
    "gold = [\n",
    "  {\n",
    "    \"_id\": \"0\",\n",
    "    \"answer\": \"The Eiffel Tower\",\n",
    "    \"supporting_facts\": \"They went to the beach.\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"1\",\n",
    "    \"answer\": \"Korea\",\n",
    "    \"supporting_facts\": \"He plays the guitar well.\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"2\",\n",
    "    \"answer\": \"France\",\n",
    "    \"supporting_facts\": \"The sun is shining brightly.\"\n",
    "  }\n",
    "]\n",
    "\n",
    "# with open('prediction_file.json', 'w') as f:\n",
    "#     json.dump(prediction, f, indent=2)\n",
    "# with open('gold_file.json', 'w') as f:\n",
    "#     json.dump(gold, f, indent=2)\n",
    "# with open('prediction_file.json') as f:\n",
    "#         prediction = json.load(f)\n",
    "# with open('gold_file.json') as f:\n",
    "#         gold = json.load(f)\n",
    "\n",
    "print(prediction)\n",
    "print(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_train으로 gold 파일 생성\n",
    "train_df = pd.DataFrame(train)[:9786]\n",
    "train_df['_id'] = train_df.index.astype(str)\n",
    "train_df.rename(columns={'question':'supporting_facts'},inplace=True)\n",
    "train_df = train_df[['_id'] + [col for col in train_df.columns if col != '_id']]\n",
    "gold = train_df.to_dict(orient='records')\n",
    "with open('gold_file.json', 'w') as f:\n",
    "    json.dump(gold, f, indent=2)\n",
    "\n",
    "# submission으로 prediction 파일 생성\n",
    "submission = pd.read_csv('submission.csv').rename(columns={'sentences':'answer','queries':'sp'}).drop(columns='id')\n",
    "prediction = submission.to_dict()\n",
    "with open('prediction_file.json', 'w') as f:\n",
    "    json.dump(prediction, f, indent=2)\n",
    "\n",
    "# 파일 읽기\n",
    "with open('gold_file.json') as f:\n",
    "        gold = json.load(f)\n",
    "with open('prediction_file.json') as f:\n",
    "        prediction = json.load(f)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>The Eiffel Tower</td>\n",
       "      <td>[[doc1, 1], [doc1, 2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q2</td>\n",
       "      <td>Korea</td>\n",
       "      <td>[[doc2, 3], [doc3, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q3</td>\n",
       "      <td>France</td>\n",
       "      <td>[[doc4, 5], [doc5, 1]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id         sentences                 queries\n",
       "0  q1  The Eiffel Tower  [[doc1, 1], [doc1, 2]]\n",
       "1  q2             Korea  [[doc2, 3], [doc3, 0]]\n",
       "2  q3            France  [[doc4, 5], [doc5, 1]]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = []\n",
    "sentences = []\n",
    "queries = []\n",
    "\n",
    "for item in gold:\n",
    "    ids.append(item['_id'])\n",
    "    sentences.append(item['answer'])\n",
    "    queries.append(item['supporting_facts'])\n",
    "\n",
    "submission = pd.DataFrame({'id': ids, 'sentences': sentences, 'queries': queries})\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1966</td>\n",
       "      <td>In what year the the venue that Marcia White i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>What country is home to the sports club loanin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jerry Clower</td>\n",
       "      <td>\"Southern Air\" featured Ray Stevens, Minnie Pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     sentences                                            queries\n",
       "0   0          1966  In what year the the venue that Marcia White i...\n",
       "1   1      Portugal  What country is home to the sports club loanin...\n",
       "2   2  Jerry Clower  \"Southern Air\" featured Ray Stevens, Minnie Pe..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [i['answer'] for i in train[:9786]]\n",
    "queries = [i['question'] for i in train[:9786]]\n",
    "id = [i for i in range(9786)]\n",
    "submission_samp = pd.DataFrame({'id':id, 'sentences':sentences,'queries':queries})\n",
    "# submission_samp.to_csv('bmission.csv',index=False)\n",
    "submission_samp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developmenting RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no chunk vector DB\n",
    "class VectorDatabase:\n",
    "    def __init__(self):\n",
    "        self.document_li = [] # original wiki data\n",
    "        self.document_embedding_li = [] # embedding wiki data\n",
    "\n",
    "    def add_document(self, document, document_embedding):\n",
    "        self.document_li.append(document)\n",
    "        self.document_embedding_li.append(document_embedding)\n",
    "\n",
    "    # Searching relevant document about embedding query\n",
    "    def search_similar_doc(self, document_li, document_embedding_li, question_embedding):\n",
    "        min_distance = float('inf') \n",
    "        best_index = None\n",
    "        for idx, sentence_embedding in enumerate(document_embedding_li):\n",
    "            # embedding된 question과 embedding된 각 wiki data의 distance 계산 \n",
    "            distance = np.linalg.norm(question_embedding - sentence_embedding)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                best_index = idx\n",
    "        # distance가 가장 작은 document와 그때의 index 반환\n",
    "        return document_li[best_index], best_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding wiki document function\n",
    "def embedding_document(document_data, model, tokenizer):\n",
    "    embeddings = [] # embedding vetor size : 256000\n",
    "    vector_db = VectorDatabase()\n",
    "    for document in tqdm(document_data):\n",
    "        inputs = tokenizer(document, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        document_embedding = outputs.logits.mean(dim=1).squeeze()\n",
    "        embeddings.append(document_embedding)\n",
    "        vector_db.add_document(document, document_embedding)\n",
    "    return embeddings, vector_db # total document embeddings\n",
    "\n",
    "def embedding_single_query(question, model, tokenizer):\n",
    "    input = tokenizer(question['question'], return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input)\n",
    "    question_embedding = outputs.logits.mean(dim=1).squeeze()\n",
    "    return question_embedding\n",
    "        \n",
    "# Chunking embedding document function\n",
    "def chunking_document(embeddings, chunk_size): #각 문장을 chunk_size로 분할\n",
    "    chunk_li = []\n",
    "    for embedding in embeddings:\n",
    "        avg = len(embedding) / float(chunk_size)\n",
    "        tmp_chunk_li = []\n",
    "        last = 0\n",
    "        while last < len(embedding):\n",
    "            tmp_chunk_li.append(embedding[int(last):int(last + avg)])\n",
    "            last += avg\n",
    "        chunk_li.append(tmp_chunk_li)\n",
    "    return chunk_li\n",
    "\n",
    "# Generate answer function\n",
    "def generate_answer(question, context, model, tokenizer):\n",
    "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(**inputs, max_length=150)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "def rag(document_data, queries, model, tokenizer):\n",
    "    # embedding_doc : embdding vector of all document \n",
    "    # vector_db : document와 embeddig document를 instance로 가지고 있는 class object\n",
    "    document_embedding, vector_db = embedding_document(document_data, model, tokenizer)\n",
    "    predictions = {\n",
    "        \"answer\": {},\n",
    "        \"sp\": {}\n",
    "    }\n",
    "    \n",
    "    for idx, query in tqdm(enumerate(queries, 1)): # start index at 1\n",
    "        question = query\n",
    "        question_embedding  = embedding_single_query(question, model, tokenizer)\n",
    "        relevant_document, relevant_index = vector_db.search_similar_doc(document_data, document_embedding, question_embedding)\n",
    "        generated_answer = generate_answer(question, relevant_document, model, tokenizer)\n",
    "\n",
    "        ans_start = generated_answer.find(\"Answer: \") + len(\"Answer: \")\n",
    "        predictions['answer']['q'+str(idx)] = generated_answer[ans_start :]\n",
    "\n",
    "        # predictions['answer']['q'+str(idx)] = generated_answer\n",
    "        predictions['sp']['q'+str(idx)] = [relevant_document, relevant_index]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testing Each Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"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\"\n",
    "login(token=hf_token)\n",
    "\n",
    "model = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-30.7039, -11.1424, -28.9968,  ..., -23.1527, -20.7024, -29.9888]), tensor([-32.2924, -10.6388, -44.6060,  ..., -24.1809, -22.5738, -31.6946]), tensor([-29.2043, -11.5534, -23.9066,  ..., -20.9144, -20.6163, -28.5592]), tensor([-31.1179, -12.0852, -22.3853,  ..., -24.5756, -21.7722, -30.4553]), tensor([-31.0465, -12.6826, -23.0472,  ..., -23.5668, -21.9608, -30.3659])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "document_embedding,vector_db = embedding_document(wiki_data_samp,model,tokenizer)\n",
    "document_li = vector_db.document_li\n",
    "document_embeddingument_li = vector_db.document_embedding_li\n",
    "print(document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-34.6664, -11.8487, -34.9489,  ..., -28.3473, -29.1205, -34.0195])\n"
     ]
    }
   ],
   "source": [
    "question_embedding = embedding_single_query(test_samp[0],model,tokenizer)\n",
    "print(question_embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2821.8289\n",
      "1973.1941\n",
      "3079.4927\n",
      "2077.254\n",
      "2297.8794\n",
      "3567.3347\n",
      "2577.3025\n",
      "2439.7722\n",
      "3198.7224\n",
      "2573.833\n",
      "best_index :  1\n"
     ]
    }
   ],
   "source": [
    "min_distance = float('inf')\n",
    "best_index = None\n",
    "for idx, doc_embedding in enumerate(document_embedding):\n",
    "    distance = np.linalg.norm(question_embedding - doc_embedding)\n",
    "    print(distance)\n",
    "    if distance < min_distance:\n",
    "        min_distance = distance\n",
    "        best_index = idx\n",
    "print('best_index : ', best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_doc: She criticiezd militay dctatorship in Pakstna and was detaide, altr dsplaced sereal tise by the urlers of that time cuh as Aybu Kan and Muhammad Zia - ul - Haq.\n",
      "\n",
      "relevant_index: 1\n"
     ]
    }
   ],
   "source": [
    "vector_db = VectorDatabase()\n",
    "relevant_document, relevant_index = vector_db.search_similar_doc(wiki_data_samp,document_embeddingument_li,question_embedding)\n",
    "print('relevant_doc:',relevant_document)\n",
    "print('relevant_index:',relevant_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Mike Huckabee took part in the most recent Republican Presidential primaries, which began on what 2016 date?\n",
      "Context: She criticiezd militay dctatorship in Pakstna and was detaide, altr dsplaced sereal tise by the urlers of that time cuh as Aybu Kan and Muhammad Zia - ul - Haq.\n",
      "\n",
      "Answer: The context does not provide any information about the date of the Republican Presidential primaries, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "answer = generate_answer(test_samp[0]['question'], relevant_document, model, tokenizer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/gayeon42/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d806ea592ea441d5b6f105a27c8f4bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_token = \"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\"\n",
    "login(token=hf_token)\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.16s/it]\n",
      "2it [00:19,  9.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': {'q1': 'I cannot answer this question from the context, as it does not provide any information about the date of the Republican Presidential primaries in 2016.',\n",
       "  'q2': 'The context does not provide any information about Pabst Brewing Company or their main brand, The National Beer of Texas. Therefore, I cannot answer this question from the provided context.'},\n",
       " 'sp': {'q1': ['She criticiezd militay dctatorship in Pakstna and was detaide, altr dsplaced sereal tise by the urlers of that time cuh as Aybu Kan and Muhammad Zia - ul - Haq.\\n',\n",
       "   1],\n",
       "  'q2': ['She criticiezd militay dctatorship in Pakstna and was detaide, altr dsplaced sereal tise by the urlers of that time cuh as Aybu Kan and Muhammad Zia - ul - Haq.\\n',\n",
       "   1]}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = rag(document_data=wiki_data_samp, \n",
    "                  queries=test_samp, \n",
    "                  model=model, tokenizer=tokenizer)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>I cannot answer this question from the context...</td>\n",
       "      <td>[She criticiezd militay dctatorship in Pakstna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q2</td>\n",
       "      <td>The context does not provide any information a...</td>\n",
       "      <td>[She criticiezd militay dctatorship in Pakstna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          sentences  \\\n",
       "0  q1  I cannot answer this question from the context...   \n",
       "1  q2  The context does not provide any information a...   \n",
       "\n",
       "                                             queries  \n",
       "0  [She criticiezd militay dctatorship in Pakstna...  \n",
       "1  [She criticiezd militay dctatorship in Pakstna...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_key = list(predictions['answer'].keys())\n",
    "answers = list(predictions['answer'].values())\n",
    "queries = list(predictions['sp'].values())\n",
    "submission = pd.DataFrame({'id':index_key,'sentences':answers,'queries':queries})\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
