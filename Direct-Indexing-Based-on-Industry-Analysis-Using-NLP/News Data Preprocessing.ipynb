{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd1d7ef",
   "metadata": {},
   "source": [
    "# 목차\n",
    "\n",
    "### 1. Import library\n",
    "\n",
    "### 2. Preprocessing Data\n",
    "- [2.1 Sector Preprocessing : 2차전지](#2.1-Sector-Preprocessing-:-2차전지)\n",
    "- [2.2 Sector Preprocessing : 바이오](#2.2-Sector-Preprocessing-:-바이오)\n",
    "- [2.3 Sector Preprocessing : 인터넷](#2.3-Sector-Preprocessing-:-인터넷)\n",
    "- [2.4 Sector Preprocessing : 게임](#2.4-Sector-Preprocessing-:-게임)\n",
    "- [2.5 종목 Preprocessing : 삼성전자](#2.5-종목-Preprocessing-:-삼성전자)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987c237",
   "metadata": {},
   "source": [
    "## 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c6e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from konlpy.tag import Kkma        \n",
    "kkma = Kkma()\n",
    "from konlpy.tag import Okt         \n",
    "t = Okt() \n",
    "from konlpy.tag import *\n",
    "import nltk\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ed2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end dates\n",
    "start_date = '2021-06'\n",
    "end_date = '2023-07'\n",
    "\n",
    "# Convert start and end dates to datetime objects\n",
    "start = pd.to_datetime(start_date)\n",
    "end = pd.to_datetime(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2991f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.readlines() # 파일을 읽어서 각 줄을 리스트의 요소로 저장(줄바꿈 문자로 저장)\n",
    "stopwords = [x.replace('\\n','') for x in stopwords] # stopword 파일의 줄바꿈 문자 제거\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf164051",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383e4e6",
   "metadata": {},
   "source": [
    "### 2.1 Sector Preprocessing : 2차전지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a0f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"2차전지\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc46cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the date range\n",
    "for date in pd.date_range(start, end, freq='M'):\n",
    "    # Generate the file name for the specific month\n",
    "    file_name = f\"./{sector}/news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    \n",
    "    # 1. Read the CSV file\n",
    "    news_data = pd.read_csv(file_name, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. Cleaning\n",
    "    regex = r'[^\\w\\s]'\n",
    "    # text 열의 모든 값을 대상으로 정규표현식을 적용하여 특수문자를 제거\n",
    "    news_data['clean_content'] = news_data['content'].apply(lambda x: re.sub(regex, '', str(x)))\n",
    "    \n",
    "    # 3. Tokenization & Pos Tagging\n",
    "    pos_tag = []\n",
    "\n",
    "    # 4. news_data 데이터프레임의 각 행에 대해 반복\n",
    "    for _, row in tqdm(news_data.iterrows()):\n",
    "        # 현재 행의 content 값을 news_text 변수에 할당\n",
    "        news_text = row['clean_content']\n",
    "\n",
    "        # PosTagging\n",
    "        tokens_ko = t.pos(news_text)\n",
    "        pos_tag.append(tokens_ko)\n",
    "    \n",
    "    # 5. Normalization\n",
    "    normalization_li = []\n",
    "    for pos in pos_tag: \n",
    "        in_li = []\n",
    "        for ele in pos:\n",
    "            #품사가 조사, 접속사이면 continue\n",
    "            if ele[1] in ['Josa','Suffix']:\n",
    "                continue\n",
    "            # 품사가 조사, 접속사가 아닌 경우, 리스트에 추가\n",
    "            in_li.append(ele[0])\n",
    "        # 한 문장의 정규화된 형태소를 정규화 리스트에 추가\n",
    "        normalization_li.append(in_li)\n",
    "        \n",
    "    # 6. Stopword Removal    \n",
    "    tokens = normalization_li\n",
    "    token_stop = []\n",
    "    # token들을 하나씩 확인\n",
    "    for token in tokens:\n",
    "        in_li = []\n",
    "        # token이 stopword 리스트에 없다면, in_li 리슽트에 추가\n",
    "        for tok in token:\n",
    "            if tok not in stopwords:\n",
    "                in_li.append(tok)\n",
    "        token_stop.append(in_li)\n",
    "\n",
    "    # 7. data save\n",
    "    df_li = []\n",
    "    # 각 문장의 token들을 공백을 기준으로 하나의 문자열로 결합\n",
    "    for tokens in token_stop:\n",
    "        token = ' '.join(tokens)\n",
    "        df_li.append(token)\n",
    "        \n",
    "    # 전처리된 데이터를 데이터프레임으로 저장\n",
    "    df = pd.DataFrame(df_li).rename(columns = {0:'preprocess_context'})\n",
    "    news_data = pd.concat([news_data,df],axis=1)\n",
    "    \n",
    "    # Generate the file name \n",
    "    file_name = f\"./{sector}_전처리/preprocess_news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    # Save the filed data to a new CSV files\n",
    "    news_data.to_csv(file_name, encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cd30e",
   "metadata": {},
   "source": [
    "### 2.2 Sector Preprocessing : 바이오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a701bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"바이오\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the date range\n",
    "for date in pd.date_range(start, end, freq='M'):\n",
    "    # Generate the file name for the specific month\n",
    "    file_name = f\"./{sector}/news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    \n",
    "    # 1. Read the CSV file\n",
    "    news_data = pd.read_csv(file_name, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. Cleaning\n",
    "    regex = r'[^\\w\\s]'\n",
    "    # text 열의 모든 값을 대상으로 정규표현식을 적용하여 특수문자를 제거\n",
    "    news_data['clean_content'] = news_data['content'].apply(lambda x: re.sub(regex, '', str(x)))\n",
    "    \n",
    "    # 3. Tokenization & Pos Tagging\n",
    "    pos_tag = []\n",
    "\n",
    "    # 4. news_data 데이터프레임의 각 행에 대해 반복\n",
    "    for _, row in tqdm(news_data.iterrows()):\n",
    "        # 현재 행의 content 값을 news_text 변수에 할당\n",
    "        news_text = row['clean_content']\n",
    "\n",
    "        # PosTagging\n",
    "        tokens_ko = t.pos(news_text)\n",
    "        pos_tag.append(tokens_ko)\n",
    "    \n",
    "    # 5. Normalization\n",
    "    normalization_li = []\n",
    "    for pos in pos_tag: \n",
    "        in_li = []\n",
    "        for ele in pos:\n",
    "            #품사가 조사, 접속사이면 continue\n",
    "            if ele[1] in ['Josa','Suffix']:\n",
    "                continue\n",
    "            in_li.append(ele[0])\n",
    "        normalization_li.append(in_li)\n",
    "        \n",
    "    # 6. Stopword Removal    \n",
    "    tokens = normalization_li\n",
    "    token_stop = []\n",
    "    for token in tokens:\n",
    "        in_li = []\n",
    "        for tok in token:\n",
    "            if tok not in stopwords:\n",
    "                in_li.append(tok)\n",
    "        token_stop.append(in_li)\n",
    "\n",
    "    # 7. data save\n",
    "    df_li = []\n",
    "    for tokens in token_stop:\n",
    "        token = ' '.join(tokens)\n",
    "        df_li.append(token)\n",
    "    df = pd.DataFrame(df_li).rename(columns = {0:'preprocess_context'})\n",
    "    news_data = pd.concat([news_data,df],axis=1)\n",
    "    \n",
    "    # Generate the file name \n",
    "    file_name = f\"./{sector}_전처리/preprocess_news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    # Save the filed data to a new CSV files\n",
    "    news_data.to_csv(file_name, encoding='utf-8-sig', index = False)\n",
    "    \n",
    "    #Display the first row of the preprocess data\n",
    "#     print(news_data[['content','preprocess_context']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3261b",
   "metadata": {},
   "source": [
    "### 2.3 Sector Preprocessing : 인터넷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "856fb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"인터넷\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the date range\n",
    "for date in pd.date_range(start, end, freq='M'):\n",
    "    # Generate the file name for the specific month\n",
    "    file_name = f\"./{sector}/news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    \n",
    "    # 1. Read the CSV file\n",
    "    news_data = pd.read_csv(file_name, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. Cleaning\n",
    "    regex = r'[^\\w\\s]'\n",
    "    # text 열의 모든 값을 대상으로 정규표현식을 적용하여 특수문자를 제거\n",
    "    news_data['clean_content'] = news_data['content'].apply(lambda x: re.sub(regex, '', str(x)))\n",
    "    \n",
    "    # 3. Tokenization & Pos Tagging\n",
    "    pos_tag = []\n",
    "\n",
    "    # 4. news_data 데이터프레임의 각 행에 대해 반복\n",
    "    for _, row in tqdm(news_data.iterrows()):\n",
    "        # 현재 행의 content 값을 news_text 변수에 할당\n",
    "        news_text = row['clean_content']\n",
    "\n",
    "        # PosTagging\n",
    "        tokens_ko = t.pos(news_text)\n",
    "        pos_tag.append(tokens_ko)\n",
    "    \n",
    "    # 5. Normalization\n",
    "    normalization_li = []\n",
    "    for pos in pos_tag: \n",
    "        in_li = []\n",
    "        for ele in pos:\n",
    "            #품사가 조사, 접속사이면 continue\n",
    "            if ele[1] in ['Josa','Suffix']:\n",
    "                continue\n",
    "            in_li.append(ele[0])\n",
    "        normalization_li.append(in_li)\n",
    "        \n",
    "    # 6. Stopword Removal    \n",
    "    tokens = normalization_li\n",
    "    token_stop = []\n",
    "    for token in tokens:\n",
    "        in_li = []\n",
    "        for tok in token:\n",
    "            if tok not in stopwords:\n",
    "                in_li.append(tok)\n",
    "        token_stop.append(in_li)\n",
    "\n",
    "    # 7. data save\n",
    "    df_li = []\n",
    "    for tokens in token_stop:\n",
    "        token = ' '.join(tokens)\n",
    "        df_li.append(token)\n",
    "    df = pd.DataFrame(df_li).rename(columns = {0:'preprocess_context'})\n",
    "    news_data = pd.concat([news_data,df],axis=1)\n",
    "    \n",
    "    # Generate the file name \n",
    "    file_name = f\"./{sector}_전처리/preprocess_news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    # Save the filed data to a new CSV files\n",
    "    news_data.to_csv(file_name, encoding='utf-8-sig', index = False)\n",
    "    \n",
    "    #Display the first row of the preprocess data\n",
    "#     print(news_data[['content','preprocess_context']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37fd2e",
   "metadata": {},
   "source": [
    "### 2.4 Sector Preprocessing : 게임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daadb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"게임\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the date range\n",
    "for date in pd.date_range(start, end, freq='M'):\n",
    "    # Generate the file name for the specific month\n",
    "    file_name = f\"./{sector}/news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    \n",
    "    # 1. Read the CSV file\n",
    "    news_data = pd.read_csv(file_name, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. Cleaning\n",
    "    regex = r'[^\\w\\s]'\n",
    "    # text 열의 모든 값을 대상으로 정규표현식을 적용하여 특수문자를 제거\n",
    "    news_data['clean_content'] = news_data['content'].apply(lambda x: re.sub(regex, '', str(x)))\n",
    "    \n",
    "    # 3. Tokenization & Pos Tagging\n",
    "    pos_tag = []\n",
    "\n",
    "    # 4. news_data 데이터프레임의 각 행에 대해 반복\n",
    "    for _, row in tqdm(news_data.iterrows()):\n",
    "        # 현재 행의 content 값을 news_text 변수에 할당\n",
    "        news_text = row['clean_content']\n",
    "\n",
    "        # PosTagging\n",
    "        tokens_ko = t.pos(news_text)\n",
    "        pos_tag.append(tokens_ko)\n",
    "    \n",
    "    # 5. Normalization\n",
    "    normalization_li = []\n",
    "    for pos in pos_tag: \n",
    "        in_li = []\n",
    "        for ele in pos:\n",
    "            #품사가 조사, 접속사이면 continue\n",
    "            if ele[1] in ['Josa','Suffix']:\n",
    "                continue\n",
    "            in_li.append(ele[0])\n",
    "        normalization_li.append(in_li)\n",
    "        \n",
    "    # 6. Stopword Removal    \n",
    "    tokens = normalization_li\n",
    "    token_stop = []\n",
    "    for token in tokens:\n",
    "        in_li = []\n",
    "        for tok in token:\n",
    "            if tok not in stopwords:\n",
    "                in_li.append(tok)\n",
    "        token_stop.append(in_li)\n",
    "\n",
    "    # 7. data save\n",
    "    df_li = []\n",
    "    for tokens in token_stop:\n",
    "        token = ' '.join(tokens)\n",
    "        df_li.append(token)\n",
    "    df = pd.DataFrame(df_li).rename(columns = {0:'preprocess_context'})\n",
    "    news_data = pd.concat([news_data,df],axis=1)\n",
    "    \n",
    "    # Generate the file name \n",
    "    file_name = f\"./{sector}_전처리/preprocess_news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    # Save the filed data to a new CSV files\n",
    "    news_data.to_csv(file_name, encoding='utf-8-sig', index = False)\n",
    "    \n",
    "    #Display the first row of the preprocess data\n",
    "#     print(news_data[['content','preprocess_context']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a391d",
   "metadata": {},
   "source": [
    "### 2.5 종목 Preprocessing : 삼성전자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "876e2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"삼성전자\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bdec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the date range\n",
    "for date in pd.date_range(start, end, freq='M'):\n",
    "    # Generate the file name for the specific month\n",
    "    file_name = f\"./{sector}/news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    \n",
    "    # 1. Read the CSV file\n",
    "    news_data = pd.read_csv(file_name, encoding='utf-8-sig')\n",
    "    \n",
    "    # 2. Cleaning\n",
    "    regex = r'[^\\w\\s]'\n",
    "    # text 열의 모든 값을 대상으로 정규표현식을 적용하여 특수문자를 제거\n",
    "    news_data['clean_content'] = news_data['content'].apply(lambda x: re.sub(regex, '', str(x)))\n",
    "    \n",
    "    # 3. Tokenization & Pos Tagging\n",
    "    pos_tag = []\n",
    "\n",
    "    # 4. news_data 데이터프레임의 각 행에 대해 반복\n",
    "    for _, row in tqdm(news_data.iterrows()):\n",
    "        # 현재 행의 content 값을 news_text 변수에 할당\n",
    "        news_text = row['clean_content']\n",
    "\n",
    "        # PosTagging\n",
    "        tokens_ko = t.pos(news_text)\n",
    "        pos_tag.append(tokens_ko)\n",
    "    \n",
    "    # 5. Normalization\n",
    "    normalization_li = []\n",
    "    for pos in pos_tag: \n",
    "        in_li = []\n",
    "        for ele in pos:\n",
    "            #품사가 조사, 접속사이면 continue\n",
    "            if ele[1] in ['Josa','Suffix']:\n",
    "                continue\n",
    "            in_li.append(ele[0])\n",
    "        normalization_li.append(in_li)\n",
    "        \n",
    "    # 6. Stopword Removal    \n",
    "    tokens = normalization_li\n",
    "    token_stop = []\n",
    "    for token in tokens:\n",
    "        in_li = []\n",
    "        for tok in token:\n",
    "            if tok not in stopwords:\n",
    "                in_li.append(tok)\n",
    "        token_stop.append(in_li)\n",
    "\n",
    "    # 7. data save\n",
    "    df_li = []\n",
    "    for tokens in token_stop:\n",
    "        token = ' '.join(tokens)\n",
    "        df_li.append(token)\n",
    "    df = pd.DataFrame(df_li).rename(columns = {0:'preprocess_context'})\n",
    "    news_data = pd.concat([news_data,df],axis=1)\n",
    "    \n",
    "    # Generate the file name \n",
    "    file_name = f\"./{sector}_전처리/preprocess_news_{date.strftime('%Y%m')}_{sector}.csv\"\n",
    "    # Save the filed data to a new CSV files\n",
    "    news_data.to_csv(file_name, encoding='utf-8-sig', index = False)\n",
    "    \n",
    "    #Display the first row of the preprocess data\n",
    "#     print(news_data[['content','preprocess_context']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textdata",
   "language": "python",
   "name": "textdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
